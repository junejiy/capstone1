{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The Project is to build a model to improve the Zestimate residual error. \n",
    "\n",
    "   $ logerror=log(Zestimate)−log(SalePrice) $ \n",
    "\n",
    "“Zestimates” are Zillow's estimated home values. The model is to predict the difference between the Zillow's estimated home value, Zestimate, and the actual sale price. \n",
    "\n",
    "## Data\n",
    "\n",
    "We have 4 files. We merge properties_2016.csv and train_2016.csv to make a training data which has 90275 data points(houses). For a test data, we merge properties_2017.csv and train_2017.csv. The test data has 77613 data points.  \n",
    "\n",
    "   **properties_2016.csv**: a full list of real estate properties in three counties (Los Angeles, Orange and Ventura, \n",
    "                           California) data in 2016.\n",
    "\n",
    "   **train_2016.csv**: all the transactions before October 15, 2016, plus some of the transactions after October \n",
    "                      15,2016. It contains parcel ID ,  transaction date and calculated log error . \n",
    "\n",
    "   **properties_2017.csv**: a full list of real estate properties in three counties (Los Angeles, Orange and Ventura,\n",
    "                           California) data in 2017.\n",
    "\n",
    "   **train_2017.csv**: all the transactions from Jan 1, 2017 to Sep 25, 2017. It can be used as a test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import a necessary module\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# read files\n",
    "\n",
    "path_properties_2016 ='/Users/kimjisun/Documents/SpringBoard/Project 1/zillow/properties_2016.csv'\n",
    "df_properties_2016 = pd.read_csv(path_properties_2016, low_memory=False)\n",
    "\n",
    "path_train_2016 ='/Users/kimjisun/Documents/SpringBoard/Project 1/zillow/train_2016.csv'\n",
    "df_train_2016 = pd.read_csv(path_train_2016 , parse_dates=[\"transactiondate\"], low_memory=False)\n",
    "\n",
    "path_properties_2017 ='/Users/kimjisun/Documents/SpringBoard/Project 1/zillow/properties_2017.csv'\n",
    "df_properties_2017 = pd.read_csv(path_properties_2017, low_memory=False)\n",
    "\n",
    "path_train_2017 ='/Users/kimjisun/Documents/SpringBoard/Project 1/zillow/train_2017.csv'\n",
    "df_train_2017 = pd.read_csv(path_train_2017 , parse_dates=[\"transactiondate\"],low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge training and properties data for 2016 and 2017\n",
    "\n",
    "df_train_2016 = pd.merge(df_train_2016, df_properties_2016, how='inner', on='parcelid')\n",
    "df_test_2017 = pd.merge(df_train_2017, df_properties_2017, how='inner', on='parcelid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2016.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2016.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "I explored training data. 125 duplicated parcelid for 2016 data and 199 duplicated parcelid for 2017 data were found. However, it meant they were trasacted for more than twice for a year. so it didn't have any problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2016.shape\n",
    "sum(df_train_2016.duplicated('parcelid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_2017.shape\n",
    "sum(df_train_2017.duplicated('parcelid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if negative values in each column. Two columns, logerror and longitude, have negative values which are reasonable. \n",
    "\n",
    "df_train_2016.describe().loc[\"min\",:][ df_train_2016.describe().loc[\"min\",:] < 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if columns with object type have unusual cells. \n",
    "\n",
    "# there are 5 columns with a object type\n",
    "\n",
    "df_train_2016.loc[:, df_train_2016.dtypes == np.object].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 5 columns to see if they have unnual cells. \n",
    "\n",
    "df_train_2016[\"hashottuborspa\"].unique() , df_train_2016[\"propertycountylandusecode\"].unique() , df_train_2016[\"propertyzoningdesc\"].unique(), df_train_2016[\"fireplaceflag\"].unique(), df_train_2016[\"taxdelinquencyflag\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "Let's check how many missing value each column has. \n",
    "\n",
    "We can find that 47 columns have missing values and 18 columns among them have more than 95% of missing values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016_missing = df_train_2016.isnull().sum(axis=0).reset_index()\n",
    "df_2016_missing.columns = [\"index\",'missing_values']\n",
    "\n",
    "\n",
    "df_2016_missing = df_2016_missing[df_2016_missing.missing_values != 0]\n",
    "df_2016_missing['ratio'] =df_2016_missing['missing_values']/len(df_train_2016)\n",
    "\n",
    "missing_values = df_2016_missing.sort_values(by='ratio', ascending=False )\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_values[missing_values['ratio']>0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw horizontal Bar plot for missing values\n",
    "\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "y_pos = np.arange(len(missing_values))\n",
    "\n",
    "\n",
    "ax.barh(y_pos, missing_values['missing_values'],   color='green')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(missing_values['index'].values)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Missing values')\n",
    "ax.set_title('missing values in each column')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "Let's draw a scatter plot on \"logerror\", then we can find that there are some outliers at the end of both sides. \n",
    "\n",
    "Our task in the project is to find where the zillow algorithm fails. These outliers means where the zillow algorithm fails the most. Thus, I will leave outliers just like that. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(df_train_2016.shape[0]), df_train_2016.sort_values(by='logerror')['logerror'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Storytelling\n",
    "\n",
    "## Distribution of Logerror\n",
    "\n",
    "We would check both logerror and absolute value of logerror. Logerror indicates wheather estimated house values has been underestimated or overestimated while absolute logerror tells us that how estimated house value is close to an actual house value. It seems like the distribution of logerror follows a normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Distribution of Logerror\n",
    "\n",
    "plt.hist(df_train_2016.logerror,50)\n",
    "plt.title(\"Distribution of Logerror\")\n",
    "plt.xlabel(\"Logerror\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of absolute Logerror\n",
    "\n",
    "plt.hist(abs(df_train_2016.logerror),50)\n",
    "plt.title(\"Distribution of absolute Logerror\")\n",
    "plt.xlabel(\"Logerror\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction dates\n",
    "\n",
    "Let's check the distribution of transaction dates, there are fewer transactions after October"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Distribution of transaction dates\n",
    "\n",
    "trs_month = df_train_2016['transactiondate'].dt.month.value_counts()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(trs_month.index, trs_month.values, alpha=0.8,color ='blue' )\n",
    "\n",
    "plt.xlabel('Month of transaction')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "Let's check correlations to \"logeror\" to see how variables are related. Square of basement and year built are the top 2 variables which have high correlations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = df_train_2016.corr()\n",
    "\n",
    "corr_logerror = corr.loc[\"logerror\",:] [ corr.loc[\"logerror\",:].notnull() ].reset_index()\n",
    "corr_logerror.columns = [\"index\",'corr']\n",
    "corr_logerror_sorted = corr_logerror.sort_values(by='corr', ascending=False )[1:]\n",
    "\n",
    "\n",
    "\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "y_pos = np.arange(len(corr_logerror_sorted))\n",
    "\n",
    "\n",
    "ax.barh(y_pos, corr_logerror_sorted['corr'],   color='green')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(corr_logerror_sorted['index'].values)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Correlation')\n",
    "ax.set_title('Correlation for logerror')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
